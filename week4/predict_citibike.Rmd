---
title: "CitiBikes"
output: html_document
date: "2023-06-19"
---

```{r setup, include=FALSE}

library(tidyverse)
library(scales)
library(modelr)
library(broom)
library (lubridate) 
theme_set(theme_bw())
options(repr.plot.width=4, repr.plot.height=3)
```

## File Inspection

```{r trips_per_day.tsv}
trips_per_day=read.table(file="trips_per_day.tsv",sep="\t",header=TRUE)
head(trips_per_day)
```
# validation set approach
Split the data into randomly selected training, validation, and test sets, with 90% of the data for training and validating the model, and 10% for a final test set (to be used once and only once towards the end of this exercise). 

When comparing possible models, you can use a single validation fold or k-fold cross-validation if you'd like a more robust estimate.
```{r train, test, validate}
set.seed(42)
num_days=nrow(trips_per_day)

#90% for training and validation; 10% for the testing
train_test_prop=0.9
ndx1=sample(1:num_days,floor(train_test_prop*num_days),replace=F)

trips_per_day_TV=trips_per_day[ndx1,]
trips_per_day_test=trips_per_day[-ndx1,]

nrow(trips_per_day_TV)
nrow(trips_per_day_test)
```


```{r train and validate}
num_daysTV=nrow(trips_per_day_TV)
frac_train=0.8
num_train=floor(num_daysTV*frac_train)

#randomly sample rows for the training set
ndx=sample(1:num_daysTV,num_train,replace = F)

#used to fit the model
trips_per_day_train=trips_per_day_TV[ndx,]

#used to evaluate the fit
trips_per_day_validate=trips_per_day_TV[-ndx,]

nrow(trips_per_day_train)
nrow(trips_per_day_validate)
```

### check the correlation between variables
```{r correlation between variables}
trips_per_day_train %>%select(-ymd,-date) %>%cor()
#it seems that num_trips are correlated to temperature 
```
```{r model fit}
K=1:8

train_err=c()
validate_err=c()

for (k in K){
  #fit on the training data
  model=lm(num_trips~poly(tmin,k,raw=T),data=trips_per_day_train)
  
  #evaluate on th training data
  train_err[k]=sqrt(mean((predict(model,trips_per_day_train)-trips_per_day_train$num_trips)^2))
  
  #evaluate on the validate data
  validate_err[k]=sqrt(mean((predict(model,trips_per_day_validate)-trips_per_day_validate$num_trips)^2))
}

#plotting
data.frame(K,train_err,validate_err)%>%
  gather("split","error",-K)%>%
  ggplot(aes(x=K,y=error,color=split))+
  geom_line()+
  scale_x_continuous(breaks=K)+
  xlab("Polynomial Degree(tmin)")+
  ylab("RMSE")


```

### model pick
It seems that the degree 4 for tmin works best b/c validate_err kind of minimizes at deg 4.
```{r model refit}
model=lm(num_trips~poly(tmin,4,raw=T),data=trips_per_day_train)

#add prediction to the training data
trips_per_day_train=trips_per_day_train%>%
  add_predictions(model)%>%
  mutate(split="train")

#add prediction data to the validate data
trips_per_day_validate=trips_per_day_validate%>%
  add_predictions(model)%>%
  mutate(split="validate")

bind_rows(trips_per_day_train,trips_per_day_validate)%>%
  ggplot(aes(x=tmin,y=num_trips))+
  geom_point(aes(color=split))+
  geom_line(aes(y=pred))+
  xlab("Min temperature")+
  ylab("Num of trips")
  
  

```
## k-fold cross-validation
The downside to a single train / validation split as done above is that when we don't have tons of data, we could get lucky (or unlucky) in terms of which rows end up in the training and validation sets.

k-fold cross-validation addresses this by first shuffling the data and then partitioning it into k "folds". The train / validation process is repeated, rotating through each fold as the validation data (and the rest as training data). This allows us to get a more stable estimate of generalization error, as well as some idea of uncertainty in that estimate.

```{r k-fold partition}
num_folds=5


trips_per_day_TV=trips_per_day_TV%>%
  mutate(fold=(row_number()%%num_folds)+1)
```

### finding the right model

```{r k-fold partition}
K=1:8
avg_val_err=c()
se_val_err=c()


for (k in K){
  
  #do a 5fold cross-val within each val of k
  val_err=c()
  for (f in 1:num_folds){
    #fit the model on the training data
    trips_per_day_train=filter(trips_per_day_TV,fold!=f)
    model=lm(num_trips~poly(tmin,k,raw=T),data=trips_per_day_train)
    
    #evaluate on the validation data
    trips_per_day_validate=filter(trips_per_day_TV,fold==f)
    val_err[f]=sqrt(
      mean((predict(model,trips_per_day_validate)-trips_per_day_validate$num_trips)^2)
    )}
    #compute the avg val error across folds
  #and the standard error on this est
    avg_val_err[k]=mean(val_err)
    se_val_err[k]=sd(val_err)/sqrt(num_folds)
    

}

#plot the validate error
data.frame(K,avg_val_err,se_val_err)%>%
  ggplot(aes(x=K,y=avg_val_err))+
  geom_pointrange(aes(ymin=avg_val_err-se_val_err,
                      ymax=avg_val_err+se_val_err,
                      color=avg_val_err==min(avg_val_err)))+
  geom_line(color="red")+
    theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab("RMES on val data")
```


```{r k-fold partition}
model=lm(num_trips~poly(tmin,4),data=trips_per_day_train)
tidy(model)

trips_per_day_test%>%
  add_predictions(model)%>%
  summarize(rmse=sqrt(mean((pred-num_trips)^2)),
                          cor=cor(pred,num_trips),
                          cor_sq=cor^2)
```
log improve the performance: apply log10 to num_trips reduce the err

```{r log10 scale}
avg_val_err=c()
se_val_err=c()


for (k in K){
  
  #do a 5fold cross-val within each val of k
  val_err=c()
  for (f in 1:num_folds){
    #fit the model on the training data
    trips_per_day_train=filter(trips_per_day_TV,fold!=f)
    model=lm(log10(num_trips)~poly(tmin,k,raw=T),data=trips_per_day_train)
    
    #evaluate on the validation data
    trips_per_day_validate=filter(trips_per_day_TV,fold==f)
    val_err[f]=sqrt(
      mean((predict(model,trips_per_day_validate)-log10(trips_per_day_validate$num_trips))^2)
    )}
    #compute the avg val error across folds
  #and the standard error on this est
    avg_val_err[k]=mean(val_err)
    se_val_err[k]=sd(val_err)/sqrt(num_folds)
    

}

#plot the validate error
data.frame(K,avg_val_err,se_val_err)%>%
  ggplot(aes(x=K,y=avg_val_err))+
  geom_pointrange(aes(ymin=avg_val_err-se_val_err,
                      ymax=avg_val_err+se_val_err,
                      color=avg_val_err==min(avg_val_err)))+
  geom_line(color="red")+
    theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')
```

## Effects of Monday, Tuesdays without Interaction

Let us investigate the effects of days(Monday, Tuesday,...) on the num of trips

```{r days+tmin, echo=FALSE}
#with interaction
trips_per_day_train=trips_per_day_train%>% 
  mutate(days=wday(as.Date(ymd),label=TRUE))

trips_per_day_test=trips_per_day_test%>% 
  mutate(days=wday(as.Date(ymd),label=TRUE))
model=lm(num_trips~days+poly(tmin,4),data=trips_per_day_train)
#tidy(model)

trips_per_day_train%>%
  add_predictions(model)%>%
  ggplot(aes(x=pred,y=num_trips))+
  geom_point()+
  geom_abline(linetype="dashed")+
  #facet_wrap(~days)+
  xlab("predicted")+
  ylab("Actual")

#seems like we are overfitting by including additional parameter
trips_per_day_test%>%
  add_predictions(model)%>%
  summarize(rmse=sqrt(mean((pred-num_trips)^2)),
                          cor=cor(pred,num_trips),
                          cor_sq=cor^2)

#rmse(model,trips_per_day_test)
#rsquare(model,trips_per_day_test)

```

let us separate the days
```{r days+tmin, echo=FALSE}
trips_per_day_train%>%
  add_predictions(model)%>%
  ggplot(aes(x=tmin,y=num_trips))+
  geom_point()+
  geom_line(aes(y=pred))+
  facet_wrap(~days)+
  xlab("tmin")+
  ylab("Num of Trips")
```


```{r days+tmin, echo=FALSE}


K=1:8
avg_val_err=c()
se_val_err=c()


trips_per_day_TV=trips_per_day_TV%>% 
  mutate(days=wday(as.Date(ymd),label=TRUE))

for (k in K){
  #do a 5fold cross-val within each val of k
  val_err=c()
  for (f in 1:num_folds){
    #fit the model on the training data
    trips_per_day_train=filter(trips_per_day_TV,fold!=f)
    model=lm(num_trips~days+poly(tmin,k,raw=T),data=trips_per_day_train)
    
    #evaluate on the validation data
    trips_per_day_validate=filter(trips_per_day_TV,fold==f)
    val_err[f]=sqrt(
      mean((predict(model,trips_per_day_validate)-trips_per_day_validate$num_trips)^2)
    )}
    #compute the avg val error across folds
  #and the standard error on this est
    avg_val_err[k]=mean(val_err)
    se_val_err[k]=sd(val_err)/sqrt(num_folds)
    

}
data.frame(K,avg_val_err,se_val_err)%>%
  ggplot(aes(x=K,y=avg_val_err))+
  geom_pointrange(aes(ymin=avg_val_err-se_val_err,
                      ymax=avg_val_err+se_val_err,
                      color=avg_val_err==min(avg_val_err)))+
  geom_line(color="red")+
    theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')



```

## Let us also consider the snowd
it seems that snowd has an impact
```{r days+tmin+snowd, echo=FALSE}
K=1:8
avg_val_err=c()
se_val_err=c()


trips_per_day_TV=trips_per_day_TV%>% 
  mutate(days=wday(as.Date(ymd),label=TRUE))

for (k in K){
  #do a 5fold cross-val within each val of k
  val_err=c()
  for (f in 1:num_folds){
    #fit the model on the training data
    trips_per_day_train=filter(trips_per_day_TV,fold!=f)
    model=lm(num_trips~days+poly(tmin,k,raw=T)*snwd,data=trips_per_day_train)
    
    #evaluate on the validation data
    trips_per_day_validate=filter(trips_per_day_TV,fold==f)
    val_err[f]=sqrt(
      mean((predict(model,trips_per_day_validate)-trips_per_day_validate$num_trips)^2)
    )}
    #compute the avg val error across folds
  #and the standard error on this est
    avg_val_err[k]=mean(val_err)
    se_val_err[k]=sd(val_err)/sqrt(num_folds)
    

}
data.frame(K,avg_val_err,se_val_err)%>%
  ggplot(aes(x=K,y=avg_val_err))+
  geom_pointrange(aes(ymin=avg_val_err-se_val_err,
                      ymax=avg_val_err+se_val_err,
                      color=avg_val_err==min(avg_val_err)))+
  geom_line(color="red")+
    theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')
```


```{r}

model=lm(num_trips~days+poly(tmin,3,raw=T)*snwd,data=trips_per_day_train)
trips_per_day_test%>%
  add_predictions(model)%>%
  summarize(rmse=sqrt(mean((pred-num_trips)^2)),
                          cor=cor(pred,num_trips),
                          cor_sq=cor^2)
```

## let us compare the models we have built so far
1. num_trips ~poly(tmin,4)
2. model=lm(num_trips~days+poly(tmin,4),data=trips_per_day_train)
3.model=lm(num_trips~days+poly(tmin,k,raw=T)*snwd,data=trips_per_day_train)
```{r rmse on test}
model1=lm(num_trips~poly(tmin,4),data=trips_per_day_train)
rmse(model1,trips_per_day_test)

model2=lm(num_trips~days+poly(tmin,4),data=trips_per_day_train)
rmse(model2,trips_per_day_test)

model3=lm(num_trips~days+poly(tmin,3,raw=T)*snwd,data=trips_per_day_train)
rmse(model3,trips_per_day_test)
```

#let us do the k-cross fold 
```{r}
model1=lm(num_trips~poly(tmin,4),data=trips_per_day_train)
rmse(model1,trips_per_day_validate)

model2=lm(num_trips~days+poly(tmin,4),data=trips_per_day_train)
rmse(model2,trips_per_day_validate)

model3=lm(num_trips~days+poly(tmin,3,raw=T)*snwd,data=trips_per_day_train)
rmse(model3,trips_per_day_validate)
```

as we can see, what is best for the validate dataset is not best for the test dataset.
